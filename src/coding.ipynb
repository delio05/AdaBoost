{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a111c3-fae0-46cd-a321-7e1748300a20",
   "metadata": {},
   "source": [
    "# 1. Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355787f3-c03a-4268-b686-f66c872569a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implementing AdaBoost for Predicting Online Shoppers' Purchase Intentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1289c-d3eb-412e-a051-b6c8d8dcd418",
   "metadata": {},
   "source": [
    "- Team: 404 Not Found\n",
    "\n",
    "- Team Members: Diksha Krishnan, Xinyu Zhou, Shen Yu, Dongyan Sun \n",
    "\n",
    "- Date: December 9, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ee17d-d94f-4ba2-bc94-87455afdae29",
   "metadata": {},
   "source": [
    "# 2. Overview of AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef68f9-a5a0-4194-9f10-128982a74c31",
   "metadata": {},
   "source": [
    "# 2.1. An overview of the algorithm and describe its advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c022814-9717-46b9-bca2-92f07a11682e",
   "metadata": {},
   "source": [
    "- AdaBoost, short for Adaptive Boosting, is a powerful ensemble learning algorithm that improves the accuracy of weak learners by combining them into a strong model. It works by iteratively training weak learners, such as decision stumps, and assigning higher weights to samples that are misclassified. \n",
    "\n",
    "- AdaBoost offers significant advantages but also comes with certain disadvantages. Its simplicity and flexibility allow it to be implemented easily with various weak learners, and it works effectively for both classification and regression problems. By focusing on misclassified samples, AdaBoost transforms weak learners that perform only slightly better than random guessing into a strong ensemble model. AdaBoost is also robust to overfitting when properly tuned and can handle non-linear problems by combining weak learners to form complex decision boundaries.\n",
    "\n",
    "- One of its disadvantages is that AdaBoost is sensitive to outliers because misclassified samples are given higher weights, which can lead to poor performance if the misclassified samples are noisy or outliers. The algorithm requires careful tuning of hyperparameters, such as the number of iterations and learning rate, to achieve optimal performance without overfitting or underfitting. The effectiveness of AdaBoost also depends on the quality of the weak learners. If the weak learners are too complex, the model may overfit the training data. Additionally, AdaBoost can be computationally intensive due to its iterative nature, particularly when applied to large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16396fe0-a382-46c1-b897-193f045f3121",
   "metadata": {},
   "source": [
    "# 2.2. Representation: describe how the feature values are converted into a single number prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9082175-9810-4367-84a9-97feb03c73f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this case of the project, the weak leaner is Disicion Stumps. AdaBoost creates a weighted combination of these stumps, resulting in a more complex decision boundary that better separates the data.\n",
    "\n",
    "Each weak learner is assigned a weight ${w_m}$, which reflects its accuracy. The weight ${w_m}$ is calculated as:\n",
    "\n",
    "\n",
    "$${w_m} = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_m}{\\epsilon_m} \\right)$$\n",
    "\n",
    "\n",
    "where $\\epsilon_m$ is the weighted error rate of the weak learner. A smaller error leads to a larger weight, indicating a better learner.\n",
    "\n",
    "The final prediction $H(x)$ of the AdaBoost model is a weighted sum of the predictions from all weak learners:\n",
    "\n",
    "\n",
    "$$H(x) = \\text{sign} \\left( \\sum_{m=1}^M {w_m} h_m(x) \\right)$$\n",
    "\n",
    "\n",
    "\n",
    "$M$: Total number of weak learners.\n",
    "\n",
    "$h_m(x)$: Prediction of the $m$-th weak learner (usually $+1$ or $-1$ for binary classification).\n",
    "\n",
    "${w_m}$: Weight of the $m$-th weak learner.\n",
    "\n",
    "$\\text{sign}$: Ensures the final output is a classification decision (e.g., $+1$ or $-1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a7b75-cda3-4c2c-9382-c88af70b15fd",
   "metadata": {},
   "source": [
    "# 2.3. Loss: describe the metric used to measure the difference between the model’s prediction and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06ec75-dd64-4c41-8c1c-8a7097744c48",
   "metadata": {},
   "source": [
    "AdaBoost minimizes an exponential loss function which ensures that large errors (misclassified points) contribute more significantly to the loss, encouraging the model to focus on correcting them.\n",
    "\n",
    "$$\\mathcal{L}(H) = \\sum_{i=1}^m \\exp(-y_i H(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84866e98-f10b-45bc-a8fa-68f2141c6ed1",
   "metadata": {},
   "source": [
    "# 2.4. Optimizer: describe the numerical algorithm used to find the model parameters that minimize the loss given a training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d21cd-d716-4183-8a7e-4c64ce9ca8b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94509cd2-f000-4d3a-933d-be078089910a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.5. Pseudo Code to explain how numerical algorithms work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45860347",
   "metadata": {},
   "source": [
    "$\\text{Input: } $\n",
    "\n",
    "$\\quad S = \\{(x_1, y_1), \\dots, (x_m, y_m)\\}, \\text{ where } y_i \\in \\{-1, +1\\},$\n",
    "\n",
    "$\\quad\\text{ number of iterations } T,$\n",
    "\n",
    "$\\quad\\text{ weak learner } WL$\n",
    "\n",
    "$\\text{Initialize: Sample weights } D_i^{(1)} = \\frac{1}{m}, \\forall i = 1, \\dots, m.$\n",
    "\n",
    "$\\quad \\textbf{for } t = 1 \\textbf{ to } T:$\n",
    "\n",
    "$\\quad\\quad \\text{Invoke weak learner } h_t=WL(D^{(t)},S) $\n",
    "\n",
    "$\\quad\\quad \\text{Compute error rate: } \\epsilon_t = \\sum_{i=1}^m D_i^{(t)} \\mathbb{1}[h_t(x_i) \\neq y_i].$\n",
    "\n",
    "$\\quad\\quad \\text{Let } w_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right).$\n",
    "\n",
    "$\\quad\\quad \\text{Update sample weights: } D_i^{(t+1)} = \\frac{D_i^{(t)} \\exp(-w_t y_i h_t(x_i))}{Z_t},$\n",
    "\n",
    "$\\quad\\quad \\text{where } Z_t = \\sum_{i=1}^m D_i^{(t)} \\exp(-w_t y_i h_t(x_i))\\text{, for all }i=1,...,m$\n",
    "\n",
    "$\\text{Output: Final hypothesis: } h_s(x) = \\text{sign}\\left(\\sum_{t=1}^T w_t h_t(x)\\right).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de13579-f68b-4e8a-88bc-071cc33497cc",
   "metadata": {},
   "source": [
    "# 2.6. References and Citations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58254d-e4cb-41dc-b81d-6200183d998b",
   "metadata": {},
   "source": [
    "- Shalev-Shwartz, S. and Ben-David, S. (2014) Understanding Machine Learning: From Theory to Algorithms. Cambridge: Cambridge University Press.\n",
    "\n",
    "- In pages 105-112 of the book, the mathematics and numerical algorithms behind AdaBoost, along with its application in face recognition, are discussed. Learned from both the book and real practice, AdaBoost relies heavily on numerical techniques such as exponentiation, logarithms, and weight normalization to ensure the algorithm’s stability and efficiency during training. One key technique is exponentiation for weight updates, where AdaBoost increases the weights of misclassified samples exponentially based on the weak classifier's error. This allows the algorithm to focus more on challenging samples in subsequent rounds. However, exponentiation can lead to very large or small values, which could cause numerical instability. To prevent issues like underflow or overflow, AdaBoost uses efficient computation of exponential functions to maintain stability across a range of values. Another important numerical technique is the logarithmic computation of alpha, which adjusts the weight given to each weak classifier based on its performance. Weak classifiers with low error rates receive higher alpha values, contributing more to the final prediction. Using logarithms to compute alpha ensures that the values remain within a manageable range, preventing excessively large or small values that could destabilize training. Finally, normalization of weights is applied after each update to prevent the weights from growing too large. This ensures that the total weight remains constant, allowing AdaBoost to continue focusing on misclassified samples without causing numerical instability. These techniques are essential for AdaBoost's reliable performance, especially on complex real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c8a4c-0beb-48f4-888c-1f7e81998bbb",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc21c6c6-d4ef-4333-9e8f-beee4a90269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add docstrings to each method and function and explain what they do and what the inputs and outputs are\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        \"\"\"\n",
    "        Initialize the AdaBoost model.\n",
    "        Args:\n",
    "            n_estimators (int): The number of weak classifiers to use in the ensemble.\n",
    "        Attributes:\n",
    "            alphas (list): Weights of each weak classifier.\n",
    "            models (list): Parameters of the weak classifiers (decision stumps).\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []  # Store the weights of the weak learners\n",
    "        self.models = []  # Store weak classifiers (decision stumps)\n",
    "\n",
    "    def _build_stump(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Build a decision stump, which is a weak classifier based on a single feature and threshold.\n",
    "        Args:\n",
    "            X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            y (numpy.ndarray): Target labels of shape (n_samples,).\n",
    "            weights (numpy.ndarray): Weights for each sample of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            stump (dict): Parameters of the best decision stump.\n",
    "            min_error (float): Minimum classification error achieved by the stump.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        min_error = float(\"inf\")  # Initialize with a high error\n",
    "        stump = {}  # To store the best stump parameters\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            # Iterate over all unique thresholds for the feature\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                for polarity in [-1, 1]:\n",
    "                    # Predict using the current threshold and polarity\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    if polarity == 1:\n",
    "                        predictions[X[:, feature] <= threshold] = -1\n",
    "                    else:\n",
    "                        predictions[X[:, feature] > threshold] = -1\n",
    "\n",
    "                    # Calculate weighted error\n",
    "                    error = np.sum(weights[predictions != y])\n",
    "                    if error < min_error:\n",
    "                        # Update the best stump parameters if error is minimized\n",
    "                        min_error = error\n",
    "                        stump['feature'] = feature\n",
    "                        stump['threshold'] = threshold\n",
    "                        stump['polarity'] = polarity\n",
    "                        stump['predictions'] = predictions\n",
    "\n",
    "        return stump, min_error\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the AdaBoost classifier using the training data.\n",
    "        Args:\n",
    "            X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            y (numpy.ndarray): Target labels of shape (n_samples,).\n",
    "\n",
    "        Modifies:\n",
    "            self.models: Appends the parameters of each weak learner.\n",
    "            self.alphas: Appends the weight of each weak learner.\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "        weights = np.ones(n_samples) / n_samples  # Initialize uniform sample weights\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Build the best weak classifier\n",
    "            stump, error = self._build_stump(X, y, weights)\n",
    "            # Compute the alpha (importance) of the stump\n",
    "            alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n",
    "            stump['alpha'] = alpha\n",
    "\n",
    "            # Update sample weights\n",
    "            weights *= np.exp(-alpha * y * stump['predictions'])\n",
    "            weights /= np.sum(weights)  # Normalize weights\n",
    "\n",
    "            # Store the stump and its weight\n",
    "            self.models.append(stump)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the given data using the trained AdaBoost model.\n",
    "        Args:\n",
    "            X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted labels of shape (n_samples,), values in {1, -1}.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        final_prediction = np.zeros(n_samples)  # Accumulate weighted predictions\n",
    "\n",
    "        for model in self.models:\n",
    "            # Extract parameters of the weak classifier\n",
    "            feature = model['feature']\n",
    "            threshold = model['threshold']\n",
    "            polarity = model['polarity']\n",
    "            alpha = model['alpha']\n",
    "\n",
    "            # Make predictions using the weak classifier\n",
    "            predictions = np.ones(n_samples)\n",
    "            if polarity == 1:\n",
    "                predictions[X[:, feature] <= threshold] = -1\n",
    "            else:\n",
    "                predictions[X[:, feature] > threshold] = -1\n",
    "\n",
    "            # Add weighted predictions to final prediction\n",
    "            final_prediction += alpha * predictions\n",
    "\n",
    "        return np.sign(final_prediction)  # Return the final ensemble prediction\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the F1 Score of the predictions.\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True labels of shape (n_samples,).\n",
    "            y_pred (numpy.ndarray): Predicted labels of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            float: F1 Score.\n",
    "        \"\"\"\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == -1) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == -1))\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-10)\n",
    "        recall = tp / (tp + fn + 1e-10)\n",
    "        return 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the accuracy of the predictions.\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True labels of shape (n_samples,).\n",
    "            y_pred (numpy.ndarray): Predicted labels of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy.\n",
    "        \"\"\"\n",
    "        return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c5645-c4f7-49d9-9425-e95e7fc77028",
   "metadata": {},
   "source": [
    "# 3. Model - Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5008208-3657-43a6-a6a4-3da902f87b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Read and preprocess the dataset from a CSV file.\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        X (numpy.ndarray): Preprocessed feature matrix.\n",
    "        y (numpy.ndarray): Target labels converted to {1, -1}.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Encode categorical features using one-hot encoding\n",
    "    categorical_features = ['OperatingSystems','Month', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\n",
    "    df = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    # Combine all numerical features for standard scaling\n",
    "    numerical_features = [\n",
    "        'Administrative', 'Informational', 'Administrative_Duration',\n",
    "        'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration', 'BounceRates',\n",
    "        'ExitRates', 'PageValues', 'SpecialDay'\n",
    "    ]\n",
    "\n",
    "    # Standard scale numerical features\n",
    "    for col in numerical_features:\n",
    "        df[col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-10)\n",
    "\n",
    "    # Separate features (X) and labels (y)\n",
    "    X = df.drop(columns=['Revenue']).values  # Feature matrix\n",
    "    y = df['Revenue'].values                 # Target labels\n",
    "    y = np.where(y == 1, 1, -1)              # Convert labels to {1, -1}\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def stratified_split(X, y):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets while maintaining class ratios.\n",
    "    Args:\n",
    "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "        y (numpy.ndarray): Target labels of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[numpy.ndarray]: (X_train, y_train, X_val, y_val, X_test, y_test).\n",
    "    \"\"\"\n",
    "    n_samples = len(y)\n",
    "    neg_indices = np.where(y == -1)[0]\n",
    "    pos_indices = np.where(y == 1)[0]\n",
    "\n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(neg_indices)\n",
    "    np.random.shuffle(pos_indices)\n",
    "\n",
    "    # Split indices by class\n",
    "    neg_train_end = int(0.6 * len(neg_indices))\n",
    "    neg_val_end = int(0.8 * len(neg_indices))\n",
    "    pos_train_end = int(0.6 * len(pos_indices))\n",
    "    pos_val_end = int(0.8 * len(pos_indices))\n",
    "\n",
    "    train_indices = np.concatenate([neg_indices[:neg_train_end], pos_indices[:pos_train_end]])\n",
    "    val_indices = np.concatenate([neg_indices[neg_train_end:neg_val_end], pos_indices[pos_train_end:pos_val_end]])\n",
    "    test_indices = np.concatenate([neg_indices[neg_val_end:], pos_indices[pos_val_end:]])\n",
    "\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    return X[train_indices], y[train_indices], X[val_indices], y[val_indices], X[test_indices], y[test_indices]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess the dataset\n",
    "    file_path = '/Users/emmasun/Desktop/2060/project/online_shoppers_intention.csv'\n",
    "    X, y = preprocess_data(file_path)\n",
    "\n",
    "    # Perform stratified split\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = stratified_split(X, y)\n",
    "\n",
    "    # Train the AdaBoost model\n",
    "    model = AdaBoost(n_estimators=50)\n",
    "    model.train(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    for split_name, (X_split, y_split) in zip(\n",
    "        ['Training', 'Validation', 'Test'],\n",
    "        [(X_train, y_train), (X_val, y_val), (X_test, y_test)]\n",
    "    ):\n",
    "        predictions = model.predict(X_split)\n",
    "        acc = model.accuracy(y_split, predictions)\n",
    "        f1 = model.f1_score(y_split, predictions)\n",
    "        print(f\"{split_name} Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f582f68-d79b-4ed6-956e-5dcc45371e94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Check Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf56e8-0589-497b-9318-f12c1ad78550",
   "metadata": {},
   "source": [
    "# 4.1. Unit Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3085c48-b233-4eff-b6cf-20e0a6c251da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc39ef5-3d7a-49ae-b199-15e36285338d",
   "metadata": {},
   "source": [
    "# 4.2. Previous work where AdaBoost is applied on a public dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2cc6ba-c8b1-40e5-a2eb-6ac436d183c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Swetha, T., R, R., Sajitha, T., B, V., Sravani, J. and Praveen, B. (2024) 'Forecasting Online Shoppers Purchase Intentions with Cat Boost Classifier', 2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT), Bengaluru, India, 2024, pp. 1-6. doi: 10.1109/ICDCOT61034.2024.10515309.\n",
    "\n",
    "- In the previous study, a system was proposed to predict online shoppers' intentions (whether they will buy or not) by analyzing various classification algorithms, including Random Forest, Decision Tree, Support Vector Machine, Gradient Boosting, AdaBoost, LightGBM, and CatBoost. The experiments were conducted using an online shoppers' intention dataset. Hyperparameter tuning was performed by first selecting an appropriate assessment metric, such as accuracy or ROC-AUC, to evaluate the model's performance. Practitioners then chose a search method, such as grid search or random search, to effectively explore the hyperparameter space. The search space for each hyperparameter was defined by a range of values or distributions. Cross-validation was employed to assess the model's performance across different hyperparameter configurations while preventing overfitting. The optimal hyperparameter values were determined by splitting the dataset into training and validation sets and iteratively training the model with different configurations. The final performance of the tuned model was evaluated on a separate test set to ensure generalization to unseen data. Experimental results showed that AdaBoost, with hyperparameter tuning, achieved an accuracy of 89.14%, while CatBoost, also with hyperparameter tuning, delivered the highest accuracy of 98.73%.\n",
    "\n",
    "- In our work, we further explore the impact of different numbers of estimators on the performance of the AdaBoost model. Specifically, we test 9 different values for n_estimators — [10, 50, 100, 150, 200, 250, 300, 350, 400] — and observe similar results. Our work achieved accuracy of 90.56% compared to 89.14%, F1 score of 66.18% compared to 63.26%, precision of 74.27% compared to 69.34%, and recall of 59.69% compared to 58.17%. This investigation helps validate the robustness of the AdaBoost model and its performance across varying configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c54c61-eb45-41a8-917c-636913f7e17f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess the dataset\n",
    "    file_path = '/Users/emmasun/Desktop/2060/project/online_shoppers_intention.csv'\n",
    "    X, y = preprocess_data(file_path)\n",
    "\n",
    "    # Perform stratified split\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = stratified_split(X, y)\n",
    "\n",
    "    # Define the list of n_estimators to try\n",
    "    n_estimators_list = [10, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "\n",
    "    # Iterate over different values of n_estimators\n",
    "    for n_estimators in n_estimators_list:\n",
    "        # Initialize and train the AdaBoost model\n",
    "        model = AdaBoost(n_estimators=n_estimators)\n",
    "        model.train(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on the test set only\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Compute the accuracy, F1 score, precision, and recall\n",
    "        acc = model.accuracy(y_test, predictions)\n",
    "        f1 = model.f1_score(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        recall = recall_score(y_test, predictions)\n",
    "\n",
    "        # Print the evaluation results for the test set\n",
    "        print(f\"Test Set Evaluation for {n_estimators} Estimators:\")\n",
    "        print(f\"Accuracy: {acc:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1d5c3-1721-4c5a-9ea3-8955bf44a2fa",
   "metadata": {},
   "source": [
    "# 5. Github repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2193e7e-b0dc-43cc-a92c-4af5a6590788",
   "metadata": {},
   "source": [
    "- https://github.com/delio05/AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ab8a2-1d95-4bbb-a82d-625b5dd1c0e8",
   "metadata": {},
   "source": [
    "# 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7773513-c615-42eb-a0a4-cd79068490d0",
   "metadata": {},
   "source": [
    "- Shalev-Shwartz, S. and Ben-David, S. (2014) Understanding Machine Learning: From Theory to Algorithms. Cambridge: Cambridge University Press.\n",
    "\n",
    "- Swetha, T., R, R., Sajitha, T., B, V., Sravani, J. and Praveen, B. (2024) 'Forecasting Online Shoppers Purchase Intentions with Cat Boost Classifier', 2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT), Bengaluru, India, 2024, pp. 1-6. doi: 10.1109/ICDCOT61034.2024.10515309.\n",
    "\n",
    "- Sakar, C. & Kastro, Y. (2018). Online Shoppers Purchasing Intention Dataset [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5F88Q.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a3450-0f44-4969-85c4-0779b8a46c9d",
   "metadata": {},
   "source": [
    "# 7. Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57374c-89bd-4d06-8bc6-db9ab3d5f2c5",
   "metadata": {},
   "source": [
    "- Thank you to Diksha Krishnan, Xinyu Zhou, Shen Yu, and Dongyan Sun for their equal contributions to the final project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
