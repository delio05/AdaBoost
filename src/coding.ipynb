{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a111c3-fae0-46cd-a321-7e1748300a20",
   "metadata": {},
   "source": [
    "# 1. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355787f3-c03a-4268-b686-f66c872569a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implementing AdaBoost for Predicting Online Shoppers' Purchase Intentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1289c-d3eb-412e-a051-b6c8d8dcd418",
   "metadata": {},
   "source": [
    "- Team: 404 Not Found\n",
    "\n",
    "- Team Members: Diksha Krishnan, Xinyu Zhou, Shen Yu, Dongyan Sun \n",
    "\n",
    "- Date: December 9, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef68f9-a5a0-4194-9f10-128982a74c31",
   "metadata": {},
   "source": [
    "# 2.1. An overview of the algorithm and describe its advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c022814-9717-46b9-bca2-92f07a11682e",
   "metadata": {},
   "source": [
    "- AdaBoost, short for Adaptive Boosting, is a powerful ensemble learning technique designed to combine multiple weak learners into a single, strong classifier in a sequential and adaptive manner. Developed by Yoav Freund and Robert Schapire, AdaBoost operates by iteratively training weak models, typically decision stumps (one-level decision trees), on a weighted version of the dataset, where the weights are adjusted dynamically after each iteration. Initially, all data points are given equal weights, but as the training progresses, higher weights are assigned to instances that were misclassified by previous learners, compelling subsequent models to focus more on these challenging examples. This adaptive weighting mechanism allows AdaBoost to progressively build a robust predictive model where each weak learner compensates for the shortcomings of its predecessors. The algorithm assigns importance to each learner based on its accuracy, combining their outputs in a weighted manner to form the final strong classifier. This makes AdaBoost particularly effective for classification tasks, as it transforms a series of weak models, each performing slightly better than random guessing, into a highly accurate predictive system. AdaBoost is widely recognized for its ability to generalize well across diverse datasets, making it a popular choice for applications such as facial recognition, medical diagnostics, fraud detection, and natural language processing. Its adaptability and focus on difficult examples give it an edge over many standalone algorithms, offering improved accuracy and better handling of complex datasets.\n",
    "\n",
    "- AdaBoost offers significant advantages but also comes with certain disadvantages. Its simplicity and flexibility allow it to be implemented easily with various weak learners, and it works effectively for both classification and regression problems. By focusing on misclassified samples, AdaBoost transforms weak learners that perform only slightly better than random guessing into a strong ensemble model. AdaBoost is also robust to overfitting when properly tuned and can handle non-linear problems by combining weak learners to form complex decision boundaries.\n",
    "\n",
    "- One of its disadvantages is that AdaBoost is sensitive to outliers because misclassified samples are given higher weights, which can lead to poor performance if the misclassified samples are noisy or outliers. The algorithm requires careful tuning of hyperparameters, such as the number of iterations and learning rate, to achieve optimal performance without overfitting or underfitting. The effectiveness of AdaBoost also depends on the quality of the weak learners. If the weak learners are too complex, the model may overfit the training data. Additionally, AdaBoost can be computationally intensive due to its iterative nature, particularly when applied to large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16396fe0-a382-46c1-b897-193f045f3121",
   "metadata": {},
   "source": [
    "# 2.2. Representation: describe how the feature values are converted into a single number prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9082175-9810-4367-84a9-97feb03c73f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the case of AdaBoost, the weak learner is the Desicion Stumps. AdaBoost creates a weighted combination of these stumps, resulting in a more complex decision boundary that better separates the data.\n",
    "\n",
    "Each weak learner is assigned a weight ${w_m}$, which reflects its accuracy. The weight ${w_m}$ is calculated as:\n",
    "\n",
    "\n",
    "$${w_m} = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_m}{\\epsilon_m} \\right)$$\n",
    "\n",
    "\n",
    "where $\\epsilon_m$ is the weighted error rate of the weak learner. A smaller error leads to a larger weight, indicating a better learner.\n",
    "\n",
    "The final prediction $H(x)$ of the AdaBoost model is a weighted sum of the predictions from all weak learners:\n",
    "\n",
    "\n",
    "$$H(x) = \\text{sign} \\left( \\sum_{m=1}^M {w_m} h_m(x) \\right)$$\n",
    "\n",
    "\n",
    "\n",
    "$M$: Total number of weak learners.\n",
    "\n",
    "$h_m(x)$: Prediction of the $m$-th weak learner (usually $+1$ or $-1$ for binary classification).\n",
    "\n",
    "${w_m}$: Weight of the $m$-th weak learner.\n",
    "\n",
    "$\\text{sign}$: Ensures the final output is a classification decision (e.g., $+1$ or $-1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a7b75-cda3-4c2c-9382-c88af70b15fd",
   "metadata": {},
   "source": [
    "# 2.3. Loss: describe the metric used to measure the difference between the modelâ€™s prediction and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06ec75-dd64-4c41-8c1c-8a7097744c48",
   "metadata": {},
   "source": [
    "AdaBoost minimizes an exponential loss function which ensures that large errors (misclassified points) contribute more significantly to the loss, encouraging the model to focus on correcting them. In this loss function, $H$ represents the model's prediction, and $y_i$ and $x_i$ are the true label and the feature vector of the i-th data point, respectively. The exponential term emphasizes the errors made by the model, meaning that misclassified points (where $y_iH(x_i)$ is negative) will contribute significantly more to the overall loss. By focusing on these larger errors, AdaBoost adjusts its model iteratively, giving more weight to the misclassified points in subsequent rounds. This process leads to improved performance on harder-to-classify examples, with the model refining its predictions to minimize these larger errors.\n",
    "\n",
    "$$\\mathcal{L}(H) = \\sum_{i=1}^m \\exp(-y_i H(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84866e98-f10b-45bc-a8fa-68f2141c6ed1",
   "metadata": {},
   "source": [
    "# 2.4. Optimizer: describe the numerical algorithm used to find the model parameters that minimize the loss given a training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d21cd-d716-4183-8a7e-4c64ce9ca8b1",
   "metadata": {},
   "source": [
    "The decision stump, as the weak learner, is a key component of the AdaBoost algorithm. It is the base model that is iteratively trained and combined to construct the final AdaBoost hypothesis. The adaptive weighting and boosting of these simple decision stumps is what allows AdaBoost to transform a collection of weak learners into a robust, accurate classifier.\n",
    "\n",
    "Let $S$ be a training set and assume that at each iteration of AdaBoost, the weak learner returns a hypothesis for which $ \\epsilon_t \\leq \\frac{1}{2} - \\gamma $. Then, the training error of the output hypothesis of AdaBoost is at most:\n",
    "\n",
    "$$L_S(h_s) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{I}[h_s(x_i) \\neq y_i] \\leq \\exp(-2 \\gamma^2 T)$$\n",
    "\n",
    "**Proof:**  \n",
    "For each *t*, denote $ f_t = \\sum_{p \\leq t} w_p h_p $. Therefore, the output of AdaBoost is $ f_T $. In addition, denote\n",
    "\n",
    "$$Z_t = \\frac{1}{m} \\sum_{i=1}^{m} e^{-y_i f_t(x_i)}$$\n",
    "\n",
    "Note that for any hypothesis, we have $\\mathbb{I}[h(x) \\neq y] \\leq e^{-y h(x)}$. Therefore $L_S(f_T) \\leq Z_T$, so it suffices to show that $ Z_T \\leq e^{-2 \\gamma^2 T} $. To upper bound $ Z_T $, we rewrite it as:\n",
    "\n",
    "$$Z_T = \\frac{Z_T}{Z_0} \\cdot \\frac{Z_T}{Z_{T-1}} \\cdot \\frac{Z_{T-1}}{Z_{T-2}} \\cdot \\dots \\cdot \\frac{Z_2}{Z_1} \\cdot \\frac{Z_1}{Z_0}$$\n",
    "\n",
    "where we used the fact that $ Z_0 = 1 $ because $ f_0 = 0 $. Therefore, it suffices to show that for every round $ t $,\n",
    "\n",
    "$$\\frac{Z_{t+1}}{Z_t} \\leq e^{-2 \\gamma^2}$$\n",
    "\n",
    "To do so, first note that using a simple inductive argument, for all $t$ and  $i$,\n",
    "\n",
    "$$D_i^{(t+1)} = \\frac{e^{-y_i f_t(x_i)}}{\\sum_{j=1}^{m} e^{-y_j f_t(x_j)}}$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\frac{Z_{t+1}}{Z_t} = \\frac{\\sum_{i=1}^{m} e^{-y_i f_{t+1}(x_i)}}{\\sum_{j=1}^{m} e^{-y_j f_t(x_j)}} = \\frac{\\sum_{i=1}^{m} e^{-y_i f_t(x_i)} e^{-y_i w_{t+1} h_{t+1}(x_i)}}{\\sum_{j=1}^{m} e^{-y_j f_t(x_j)}} = e^{-w_t} \\left( \\sum_{i : y_i h_{t+1}(x_i) = 1} D_i^{(t+1)} + e^{w_t} \\sum_{i : y_i h_{t+1}(x_i) = -1} D_i^{(t+1)} \\right)$$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$$e^{-w_t} \\left( 1 - \\epsilon_t + e^{w_t} \\epsilon_{t+1} \\right)$$\n",
    "\n",
    "By our assumption, $ \\epsilon_{t+1} \\leq \\frac{1}{2} - \\gamma$. Since the function $ g(a) = a(1 - a) $ is monotonically increasing in $ [0, 1/2] $, we obtain that:\n",
    "\n",
    "$$2 \\sqrt{\\epsilon_t(1 - \\epsilon_{t+1})} \\leq 2 \\sqrt{\\left( \\frac{1}{2} - \\gamma \\right) \\left( \\frac{1}{2} + \\gamma \\right)} = \\sqrt{1 - 4 \\gamma^2}$$\n",
    "\n",
    "The proof shows that AdaBoost's training error is exponentially bounded by $ \\exp(-2 \\gamma^2 T) $, where $ \\gamma $ is the margin of the weak learners and $ T $ is the number of iterations. The key idea is to bound the normalization factor $ Z_T $, which ensures that AdaBoost minimizes error during each iteration. By expressing $ Z_T $ in terms of previous normalization factors and using an inductive argument, the proof demonstrates that $ Z_{t+1}/Z_t $ decreases exponentially at each step. This reduction is driven by the weak learnerâ€™s error rate, $ \\epsilon_t $, and the argument uses a bound on $ \\epsilon_t $ to show that the error of AdaBoost decreases over time. Thus, AdaBoostâ€™s output hypothesis improves with each iteration, leading to an exponentially decaying training error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94509cd2-f000-4d3a-933d-be078089910a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.5. Pseudo Code to explain how numerical algorithms work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45860347",
   "metadata": {},
   "source": [
    "$\\text{Input: } $\n",
    "\n",
    "$\\quad S = \\{(x_1, y_1), \\dots, (x_m, y_m)\\}, \\text{ where } y_i \\in \\{-1, +1\\},$\n",
    "\n",
    "$\\quad\\text{ number of iterations } T,$\n",
    "\n",
    "$\\quad\\text{ weak learner } WL$\n",
    "\n",
    "$\\text{Initialize: Sample weights } D_i^{(1)} = \\frac{1}{m}, \\forall i = 1, \\dots, m.$\n",
    "\n",
    "$\\quad \\textbf{for } t = 1 \\textbf{ to } T:$\n",
    "\n",
    "$\\quad\\quad \\text{Invoke weak learner } h_t=WL(D^{(t)},S) $\n",
    "\n",
    "$\\quad\\quad \\text{Compute error rate: } \\epsilon_t = \\sum_{i=1}^m D_i^{(t)} \\mathbb{1}[h_t(x_i) \\neq y_i].$\n",
    "\n",
    "$\\quad\\quad \\text{Let } w_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right).$\n",
    "\n",
    "$\\quad\\quad \\text{Update sample weights: } D_i^{(t+1)} = \\frac{D_i^{(t)} \\exp(-w_t y_i h_t(x_i))}{Z_t},$\n",
    "\n",
    "$\\quad\\quad \\text{where } Z_t = \\sum_{i=1}^m D_i^{(t)} \\exp(-w_t y_i h_t(x_i))\\text{, for all }i=1,...,m$\n",
    "\n",
    "$\\text{Output: Final hypothesis: } h_s(x) = \\text{sign}\\left(\\sum_{t=1}^T w_t h_t(x)\\right).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de13579-f68b-4e8a-88bc-071cc33497cc",
   "metadata": {},
   "source": [
    "# 2.6. References and Citations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58254d-e4cb-41dc-b81d-6200183d998b",
   "metadata": {},
   "source": [
    "- Shalev-Shwartz, S. and Ben-David, S. (2014) Understanding Machine Learning: From Theory to Algorithms. Cambridge: Cambridge University Press.\n",
    "\n",
    "- In pages 105-112 of the book, the mathematics and numerical algorithms behind AdaBoost, along with its application in face recognition, are discussed. Learned from both the book and real practice, AdaBoost relies heavily on numerical techniques such as exponentiation, logarithms, and weight normalization to ensure the algorithmâ€™s stability and efficiency during training. One key technique is exponentiation for weight updates, where AdaBoost increases the weights of misclassified samples exponentially based on the weak classifier's error. This allows the algorithm to focus more on challenging samples in subsequent rounds. However, exponentiation can lead to very large or small values, which could cause numerical instability. To prevent issues like underflow or overflow, AdaBoost uses efficient computation of exponential functions to maintain stability across a range of values. Another important numerical technique is the logarithmic computation of alpha, which adjusts the weight given to each weak classifier based on its performance. Weak classifiers with low error rates receive higher alpha values, contributing more to the final prediction. Using logarithms to compute alpha ensures that the values remain within a manageable range, preventing excessively large or small values that could destabilize training. Finally, normalization of weights is applied after each update to prevent the weights from growing too large. This ensures that the total weight remains constant, allowing AdaBoost to continue focusing on misclassified samples without causing numerical instability. These techniques are essential for AdaBoost's reliable performance, especially on complex real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c8a4c-0beb-48f4-888c-1f7e81998bbb",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc21c6c6-d4ef-4333-9e8f-beee4a90269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add docstrings to each method and function and explain what they do and what the inputs and outputs are\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        \"\"\"\n",
    "        Initialize the AdaBoost model.\n",
    "        Args:\n",
    "            n_estimators (int): The number of weak classifiers to use in the ensemble.\n",
    "        Attributes:\n",
    "            alphas (list): Weights of each weak classifier.\n",
    "            models (list): Parameters of the weak classifiers (decision stumps).\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []  # Store the weights of the weak learners\n",
    "        self.models = []  # Store weak classifiers (decision stumps)\n",
    "\n",
    "    def _build_stump(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Build a decision stump, which is a weak classifier based on a single feature and threshold.\n",
    "        Args:\n",
    "            X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            y (numpy.ndarray): Target labels of shape (n_samples,).\n",
    "            weights (numpy.ndarray): Weights for each sample of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            stump (dict): Parameters of the best decision stump.\n",
    "            min_error (float): Minimum classification error achieved by the stump.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        min_error = float(\"inf\")  # Initialize with a high error\n",
    "        stump = {}  # To store the best stump parameters\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            # Iterate over all unique thresholds for the feature\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                for polarity in [-1, 1]:\n",
    "                    # Predict using the current threshold and polarity\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    if polarity == 1:\n",
    "                        predictions[X[:, feature] <= threshold] = -1\n",
    "                    else:\n",
    "                        predictions[X[:, feature] > threshold] = -1\n",
    "\n",
    "                    # Calculate weighted error\n",
    "                    error = np.sum(weights[predictions != y])\n",
    "                    if error < min_error:\n",
    "                        # Update the best stump parameters if error is minimized\n",
    "                        min_error = error\n",
    "                        stump['feature'] = feature\n",
    "                        stump['threshold'] = threshold\n",
    "                        stump['polarity'] = polarity\n",
    "                        stump['predictions'] = predictions\n",
    "\n",
    "        return stump, min_error\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the AdaBoost classifier using the training data.\n",
    "        Args:\n",
    "            X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            y (numpy.ndarray): Target labels of shape (n_samples,).\n",
    "\n",
    "        Modifies:\n",
    "            self.models: Appends the parameters of each weak learner.\n",
    "            self.alphas: Appends the weight of each weak learner.\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "        if n_samples <= 1:\n",
    "            return\n",
    "        weights = np.ones(n_samples) / n_samples  # Initialize uniform sample weights\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Build the best weak classifier\n",
    "            stump, error = self._build_stump(X, y, weights)\n",
    "            # Compute the alpha (importance) of the stump\n",
    "            alpha = 0.5 * np.log((1 - error) / (error + 1e-10))\n",
    "            stump['alpha'] = alpha\n",
    "\n",
    "            # Update sample weights\n",
    "            weights *= np.exp(-alpha * y * stump['predictions'])\n",
    "            weights /= np.sum(weights)  # Normalize weights\n",
    "\n",
    "            # Store the stump and its weight\n",
    "            self.models.append(stump)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the given data using the trained AdaBoost model.\n",
    "        Args:\n",
    "            X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted labels of shape (n_samples,), values in {1, -1}.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        final_prediction = np.zeros(n_samples)  # Accumulate weighted predictions\n",
    "\n",
    "        for model in self.models:\n",
    "            # Extract parameters of the weak classifier\n",
    "            feature = model['feature']\n",
    "            threshold = model['threshold']\n",
    "            polarity = model['polarity']\n",
    "            alpha = model['alpha']\n",
    "\n",
    "            # Make predictions using the weak classifier\n",
    "            predictions = np.ones(n_samples)\n",
    "            if polarity == 1:\n",
    "                predictions[X[:, feature] <= threshold] = -1\n",
    "            else:\n",
    "                predictions[X[:, feature] > threshold] = -1\n",
    "\n",
    "            # Add weighted predictions to final prediction\n",
    "            final_prediction += alpha * predictions\n",
    "\n",
    "        return np.sign(final_prediction)  # Return the final ensemble prediction\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the F1 Score of the predictions.\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True labels of shape (n_samples,).\n",
    "            y_pred (numpy.ndarray): Predicted labels of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            float: F1 Score.\n",
    "        \"\"\"\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == -1) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == -1))\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-10)\n",
    "        recall = tp / (tp + fn + 1e-10)\n",
    "        return 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the accuracy of the predictions.\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True labels of shape (n_samples,).\n",
    "            y_pred (numpy.ndarray): Predicted labels of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy.\n",
    "        \"\"\"\n",
    "        return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c5645-c4f7-49d9-9425-e95e7fc77028",
   "metadata": {},
   "source": [
    "# 3. Model - Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5008208-3657-43a6-a6a4-3da902f87b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8981, F1 Score: 0.6396\n",
      "Validation Accuracy: 0.8865, F1 Score: 0.6078\n",
      "Test Accuracy: 0.8950, F1 Score: 0.6357\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Read and preprocess the dataset from a CSV file.\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        X (numpy.ndarray): Preprocessed feature matrix.\n",
    "        y (numpy.ndarray): Target labels converted to {1, -1}.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Encode categorical features using one-hot encoding\n",
    "    categorical_features = ['OperatingSystems','Month', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\n",
    "    df = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    # Combine all numerical features for standard scaling\n",
    "    numerical_features = [\n",
    "        'Administrative', 'Informational', 'Administrative_Duration',\n",
    "        'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration', 'BounceRates',\n",
    "        'ExitRates', 'PageValues', 'SpecialDay'\n",
    "    ]\n",
    "\n",
    "    # Standard scale numerical features\n",
    "    for col in numerical_features:\n",
    "        df[col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-10)\n",
    "\n",
    "    # Separate features (X) and labels (y)\n",
    "    X = df.drop(columns=['Revenue']).values  # Feature matrix\n",
    "    y = df['Revenue'].values                 # Target labels\n",
    "    y = np.where(y == 1, 1, -1)              # Convert labels to {1, -1}\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def stratified_split(X, y):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets while maintaining class ratios.\n",
    "    Args:\n",
    "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "        y (numpy.ndarray): Target labels of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[numpy.ndarray]: (X_train, y_train, X_val, y_val, X_test, y_test).\n",
    "    \"\"\"\n",
    "    n_samples = len(y)\n",
    "    neg_indices = np.where(y == -1)[0]\n",
    "    pos_indices = np.where(y == 1)[0]\n",
    "\n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(neg_indices)\n",
    "    np.random.shuffle(pos_indices)\n",
    "\n",
    "    # Split indices by class\n",
    "    neg_train_end = int(0.6 * len(neg_indices))\n",
    "    neg_val_end = int(0.8 * len(neg_indices))\n",
    "    pos_train_end = int(0.6 * len(pos_indices))\n",
    "    pos_val_end = int(0.8 * len(pos_indices))\n",
    "\n",
    "    train_indices = np.concatenate([neg_indices[:neg_train_end], pos_indices[:pos_train_end]])\n",
    "    val_indices = np.concatenate([neg_indices[neg_train_end:neg_val_end], pos_indices[pos_train_end:pos_val_end]])\n",
    "    test_indices = np.concatenate([neg_indices[neg_val_end:], pos_indices[pos_val_end:]])\n",
    "\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    return X[train_indices], y[train_indices], X[val_indices], y[val_indices], X[test_indices], y[test_indices]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess the dataset\n",
    "    file_path = '/Users/emmasun/Desktop/2060/project/online_shoppers_intention.csv'\n",
    "    X, y = preprocess_data(file_path)\n",
    "\n",
    "    # Perform stratified split\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = stratified_split(X, y)\n",
    "\n",
    "    # Train the AdaBoost model\n",
    "    model = AdaBoost(n_estimators=50)\n",
    "    model.train(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    for split_name, (X_split, y_split) in zip(\n",
    "        ['Training', 'Validation', 'Test'],\n",
    "        [(X_train, y_train), (X_val, y_val), (X_test, y_test)]\n",
    "    ):\n",
    "        predictions = model.predict(X_split)\n",
    "        acc = model.accuracy(y_split, predictions)\n",
    "        f1 = model.f1_score(y_split, predictions)\n",
    "        print(f\"{split_name} Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f582f68-d79b-4ed6-956e-5dcc45371e94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Check Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf56e8-0589-497b-9318-f12c1ad78550",
   "metadata": {},
   "source": [
    "# 4.1. Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437c7ece-cc2a-49b1-9fd2-976340a09f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_build_stump passed!\n",
      "test_predict passed!\n",
      "test_evaluation_metrics passed!\n",
      "test_large_datasets passed!\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Helper function for generating synthetic data\n",
    "def generate_synthetic_data(n_samples, n_features, imbalance_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Generate synthetic binary classification data.\n",
    "    Args:\n",
    "        n_samples (int): Number of samples.\n",
    "        n_features (int): Number of features.\n",
    "        imbalance_ratio (float): Proportion of positive samples.\n",
    "    Returns:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target labels.\n",
    "    \"\"\"\n",
    "    X = np.random.rand(n_samples, n_features)\n",
    "    y = np.random.choice([1, -1], size=n_samples, p=[imbalance_ratio, 1 - imbalance_ratio])\n",
    "    return X, y\n",
    "\n",
    "# Unit test for `_build_stump` method\n",
    "def test_build_stump():\n",
    "    \"\"\"\n",
    "    Test `_build_stump` method.\n",
    "    Goals:\n",
    "    - Validate correct selection of feature, threshold, and polarity.\n",
    "    - Handle edge cases like tied feature values, single sample, and extreme weights.\n",
    "    \"\"\"\n",
    "    model = AdaBoost()\n",
    "\n",
    "    # Case 1: Tied feature values\n",
    "    # Goal: Ensure stump can handle identical feature values correctly.\n",
    "    X = np.array([[1], [1], [1], [1]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    weights = np.ones(4) / 4\n",
    "    stump, error = model._build_stump(X, y, weights)\n",
    "    assert stump[\"threshold\"] == 1, \"Threshold mismatch for tied feature values\"\n",
    "    assert error == 0.5, f\"Error mismatch: {error}\"\n",
    "\n",
    "    # Case 2: Single sample\n",
    "    # Goal: Verify behavior when only one sample is present.\n",
    "    X = np.array([[2]])\n",
    "    y = np.array([1])\n",
    "    weights = np.array([1])\n",
    "    stump, error = model._build_stump(X, y, weights)\n",
    "    assert stump[\"feature\"] == 0, \"Feature mismatch for single sample\"\n",
    "    assert stump[\"threshold\"] == 2, \"Threshold mismatch for single sample\"\n",
    "    assert error == 0, \"Error mismatch for single sample\"\n",
    "\n",
    "    # Case 3: Extreme weights\n",
    "    # Goal: Test if the stump handles very small and very large weights.\n",
    "    X = np.array([[1], [2], [3], [4]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    weights = np.array([1e-10, 1e-10, 1e-10, 1.0])\n",
    "    stump, error = model._build_stump(X, y, weights)\n",
    "    assert error < 1, f\"Error out of range: {error}\"\n",
    "\n",
    "    print(\"test_build_stump passed!\")\n",
    "\n",
    "# Unit test for `predict` method\n",
    "def test_predict():\n",
    "    \"\"\"\n",
    "    Test the `predict` method.\n",
    "    Goals:\n",
    "    - Verify correctness and robustness across various edge cases.\n",
    "    \"\"\"\n",
    "    # Case 1: Near decision boundaries\n",
    "    # Goal: Test predictions for samples near decision boundaries.\n",
    "    X_train = np.array([[1], [2], [3], [4]])\n",
    "    y_train = np.array([1, 1, -1, -1])\n",
    "    X_test = np.array([[1.5], [3.5]])\n",
    "    model = AdaBoost(n_estimators=5)\n",
    "    model.train(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    assert predictions.shape == (X_test.shape[0],), \"Prediction shape mismatch near boundaries\"\n",
    "    assert np.all(np.isin(predictions, [-1, 1])), \"Invalid prediction values near boundaries\"\n",
    "\n",
    "    # Case 2: Extreme values\n",
    "    # Goal: Test predictions for unseen extreme feature values.\n",
    "    X_test = np.array([[100], [-100]])\n",
    "    predictions = model.predict(X_test)\n",
    "    assert predictions.shape == (X_test.shape[0],), \"Prediction shape mismatch for extreme values\"\n",
    "\n",
    "    # Case 3: Empty dataset\n",
    "    # Goal: Verify behavior with an empty dataset.\n",
    "    X_test = np.empty((0, 1))\n",
    "    predictions = model.predict(X_test)\n",
    "    assert predictions.shape == (0,), \"Prediction shape mismatch for empty dataset\"\n",
    "\n",
    "    # Case 4: Multi-dimensional features\n",
    "    # Goal: Test predictions for multi-dimensional input features.\n",
    "    X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "    y_train = np.array([1, 1, -1, -1])\n",
    "    X_test = np.array([[2, 3], [6, 7]])\n",
    "    model = AdaBoost(n_estimators=5)\n",
    "    model.train(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    assert predictions.shape == (X_test.shape[0],), \"Prediction shape mismatch for multi-dimensional features\"\n",
    "\n",
    "    print(\"test_predict passed!\")\n",
    "\n",
    "# Unit test for evaluation metrics\n",
    "def test_evaluation_metrics():\n",
    "    \"\"\"\n",
    "    Test `accuracy` and `f1_score` methods.\n",
    "    Goals:\n",
    "    - Verify correctness for perfect and partially incorrect predictions.\n",
    "    \"\"\"\n",
    "    model = AdaBoost()\n",
    "\n",
    "    y_true = np.array([1, 1, -1, -1])\n",
    "    y_pred_perfect = np.array([1, 1, -1, -1])\n",
    "    y_pred_partial = np.array([1, -1, -1, 1])\n",
    "\n",
    "    # Perfect predictions\n",
    "    assert abs(model.accuracy(y_true, y_pred_perfect) - 1.0) < 1e-6, \"Accuracy failed for perfect predictions\"\n",
    "    assert abs(model.f1_score(y_true, y_pred_perfect) - 1.0) < 1e-6, \"F1 score failed for perfect predictions\"\n",
    "\n",
    "    # Partially incorrect predictions\n",
    "    assert abs(model.accuracy(y_true, y_pred_partial) - 0.5) < 1e-6, \"Accuracy mismatch for partially incorrect data\"\n",
    "    assert abs(model.f1_score(y_true, y_pred_partial) - 0.5) < 1e-6, \"F1 score mismatch for partially incorrect data\"\n",
    "\n",
    "    print(\"test_evaluation_metrics passed!\")\n",
    "\n",
    "# Unit test for large datasets\n",
    "def test_large_datasets():\n",
    "    \"\"\"\n",
    "    Test model scalability and performance on large datasets.\n",
    "    Goals:\n",
    "    - Validate training and prediction with large sample sizes.\n",
    "    \"\"\"\n",
    "    X, y = generate_synthetic_data(5000, 5)\n",
    "    model = AdaBoost(n_estimators=20)\n",
    "    model.train(X, y)\n",
    "    predictions = model.predict(X)\n",
    "    assert predictions.shape == (X.shape[0],), \"Prediction shape mismatch for large dataset\"\n",
    "    print(\"test_large_datasets passed!\")\n",
    "\n",
    "# Run all unit tests\n",
    "def run_all_tests():\n",
    "    test_build_stump()\n",
    "    test_predict()\n",
    "    test_evaluation_metrics()\n",
    "    test_large_datasets()\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Execute the tests\n",
    "run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc39ef5-3d7a-49ae-b199-15e36285338d",
   "metadata": {},
   "source": [
    "# 4.2. Previous work where AdaBoost is applied on a public dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2cc6ba-c8b1-40e5-a2eb-6ac436d183c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Swetha, T., R, R., Sajitha, T., B, V., Sravani, J. and Praveen, B. (2024) 'Forecasting Online Shoppers Purchase Intentions with Cat Boost Classifier', 2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT), Bengaluru, India, 2024, pp. 1-6. doi: 10.1109/ICDCOT61034.2024.10515309.\n",
    "\n",
    "- In the previous study, a system was proposed to predict online shoppers' intentions (whether they will buy or not) by analyzing various classification algorithms, including Random Forest, Decision Tree, Support Vector Machine, Gradient Boosting, AdaBoost, LightGBM, and CatBoost. The experiments were conducted using an online shoppers' intention dataset. Hyperparameter tuning was performed by first selecting an appropriate assessment metric, such as accuracy or ROC-AUC, to evaluate the model's performance. Practitioners then chose a search method, such as grid search or random search, to effectively explore the hyperparameter space. The search space for each hyperparameter was defined by a range of values or distributions. Cross-validation was employed to assess the model's performance across different hyperparameter configurations while preventing overfitting. The optimal hyperparameter values were determined by splitting the dataset into training and validation sets and iteratively training the model with different configurations. The final performance of the tuned model was evaluated on a separate test set to ensure generalization to unseen data. Experimental results showed that AdaBoost, with hyperparameter tuning, achieved an accuracy of 89.14%, while CatBoost, also with hyperparameter tuning, delivered the highest accuracy of 98.73%.\n",
    "\n",
    "- In our work, we further explore the impact of different numbers of estimators on the performance of the AdaBoost model. Specifically, we test 9 different values for n_estimators â€” [10, 50, 100, 150, 200, 250, 300, 350, 400] â€” and observe similar results. Our work achieved accuracy of 90.56% compared to 89.14%, F1 score of 66.18% compared to 63.26%, precision of 74.27% compared to 69.34%, and recall of 59.69% compared to 58.17%. This investigation helps validate the robustness of the AdaBoost model and its performance across varying configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c54c61-eb45-41a8-917c-636913f7e17f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation for 10 Estimators:\n",
      "Accuracy: 0.8877, F1 Score: 0.6137, Precision: 0.6567, Recall: 0.5759\n",
      "--------------------------------------------------\n",
      "Test Set Evaluation for 50 Estimators:\n",
      "Accuracy: 0.8897, F1 Score: 0.6047, Precision: 0.6797, Recall: 0.5445\n",
      "--------------------------------------------------\n",
      "Test Set Evaluation for 100 Estimators:\n",
      "Accuracy: 0.8881, F1 Score: 0.5988, Precision: 0.6732, Recall: 0.5393\n",
      "--------------------------------------------------\n",
      "Test Set Evaluation for 150 Estimators:\n",
      "Accuracy: 0.8877, F1 Score: 0.5908, Precision: 0.6780, Recall: 0.5236\n",
      "--------------------------------------------------\n",
      "Test Set Evaluation for 200 Estimators:\n",
      "Accuracy: 0.8873, F1 Score: 0.5888, Precision: 0.6769, Recall: 0.5209\n",
      "--------------------------------------------------\n",
      "Test Set Evaluation for 250 Estimators:\n",
      "Accuracy: 0.8869, F1 Score: 0.5854, Precision: 0.6770, Recall: 0.5157\n",
      "--------------------------------------------------\n",
      "Test Set Evaluation for 300 Estimators:\n",
      "Accuracy: 0.8857, F1 Score: 0.5816, Precision: 0.6712, Recall: 0.5131\n",
      "--------------------------------------------------\n",
      "Test Set Evaluation for 350 Estimators:\n",
      "Accuracy: 0.8837, F1 Score: 0.5723, Precision: 0.6644, Recall: 0.5026\n",
      "--------------------------------------------------\n",
      "Test Set Evaluation for 400 Estimators:\n",
      "Accuracy: 0.8857, F1 Score: 0.5816, Precision: 0.6712, Recall: 0.5131\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess the dataset\n",
    "    file_path = '/Users/emmasun/Desktop/2060/project/online_shoppers_intention.csv'\n",
    "    X, y = preprocess_data(file_path)\n",
    "\n",
    "    # Perform stratified split\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = stratified_split(X, y)\n",
    "\n",
    "    # Define the list of n_estimators to try\n",
    "    n_estimators_list = [10, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "\n",
    "    # Iterate over different values of n_estimators\n",
    "    for n_estimators in n_estimators_list:\n",
    "        # Initialize and train the AdaBoost model\n",
    "        model = AdaBoost(n_estimators=n_estimators)\n",
    "        model.train(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model on the test set only\n",
    "        predictions = model.predict(X_test)\n",
    "\n",
    "        # Compute the accuracy, F1 score, precision, and recall\n",
    "        acc = model.accuracy(y_test, predictions)\n",
    "        f1 = model.f1_score(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        recall = recall_score(y_test, predictions)\n",
    "\n",
    "        # Print the evaluation results for the test set\n",
    "        print(f\"Test Set Evaluation for {n_estimators} Estimators:\")\n",
    "        print(f\"Accuracy: {acc:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1d5c3-1721-4c5a-9ea3-8955bf44a2fa",
   "metadata": {},
   "source": [
    "# 5. Github repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2193e7e-b0dc-43cc-a92c-4af5a6590788",
   "metadata": {},
   "source": [
    "- https://github.com/delio05/AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ab8a2-1d95-4bbb-a82d-625b5dd1c0e8",
   "metadata": {},
   "source": [
    "# 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7773513-c615-42eb-a0a4-cd79068490d0",
   "metadata": {},
   "source": [
    "- Shalev-Shwartz, S. and Ben-David, S. (2014) Understanding Machine Learning: From Theory to Algorithms. Cambridge: Cambridge University Press.\n",
    "\n",
    "- Swetha, T., R, R., Sajitha, T., B, V., Sravani, J. and Praveen, B. (2024) 'Forecasting Online Shoppers Purchase Intentions with Cat Boost Classifier', 2024 International Conference on Distributed Computing and Optimization Techniques (ICDCOT), Bengaluru, India, 2024, pp. 1-6. doi: 10.1109/ICDCOT61034.2024.10515309.\n",
    "\n",
    "- Sakar, C. & Kastro, Y. (2018). Online Shoppers Purchasing Intention Dataset [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5F88Q.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a3450-0f44-4969-85c4-0779b8a46c9d",
   "metadata": {},
   "source": [
    "# 7. Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57374c-89bd-4d06-8bc6-db9ab3d5f2c5",
   "metadata": {},
   "source": [
    "- Thank you to Diksha Krishnan, Xinyu Zhou, Shen Yu, and Dongyan Sun for their equal contributions to the final project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
